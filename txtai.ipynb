{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit86f9c05695d848ada93bacbefaaa208e",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting txtai\n  Downloading txtai-1.1.0-py3-none-any.whl (19 kB)\nRequirement already satisfied: numpy>=1.18.4 in /home/antony/.local/lib/python3.8/site-packages (from txtai) (1.19.0)\nCollecting scikit-learn>=0.23.1\n  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n\u001b[K     |████████████████████████████████| 6.8 MB 1.9 MB/s \n\u001b[?25hCollecting torch>=1.4.0\n  Downloading torch-1.6.0-cp38-cp38-manylinux1_x86_64.whl (748.8 MB)\n\u001b[K     |████████████████████████████████| 748.8 MB 35 kB/s \n\u001b[?25hCollecting sentence-transformers>=0.3.3\n  Downloading sentence-transformers-0.3.4.tar.gz (61 kB)\n\u001b[K     |████████████████████████████████| 61 kB 193 kB/s \n\u001b[?25hCollecting annoy>=1.16.3\n  Downloading annoy-1.16.3.tar.gz (644 kB)\n\u001b[K     |████████████████████████████████| 644 kB 2.0 MB/s \n\u001b[?25hCollecting regex>=2020.5.14\n  Downloading regex-2020.7.14-cp38-cp38-manylinux2010_x86_64.whl (672 kB)\n\u001b[K     |████████████████████████████████| 672 kB 2.0 MB/s \n\u001b[?25hCollecting nltk>=3.5\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 1.7 MB/s \n\u001b[?25hCollecting pymagnitude-lite>=0.1.43\n  Downloading pymagnitude_lite-0.1.143-py3-none-any.whl (34 kB)\nCollecting hnswlib>=0.4.0\n  Downloading hnswlib-0.4.0.tar.gz (17 kB)\nCollecting fasttext>=0.9.2\n  Downloading fasttext-0.9.2.tar.gz (68 kB)\n\u001b[K     |████████████████████████████████| 68 kB 1.9 MB/s \n\u001b[?25hCollecting transformers>=3.0.2\n  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n\u001b[K     |████████████████████████████████| 769 kB 2.1 MB/s \n\u001b[?25hCollecting faiss-cpu>=1.6.3; os_name != \"nt\"\n  Downloading faiss_cpu-1.6.3-cp38-cp38-manylinux2010_x86_64.whl (7.2 MB)\n\u001b[K     |████████████████████████████████| 7.2 MB 2.1 MB/s \n\u001b[?25hCollecting tqdm>=4.46.0\n  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n\u001b[K     |████████████████████████████████| 68 kB 1.9 MB/s \n\u001b[?25hCollecting joblib>=0.11\n  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n\u001b[K     |████████████████████████████████| 300 kB 2.0 MB/s \n\u001b[?25hCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\nCollecting scipy>=0.19.1\n  Downloading scipy-1.5.2-cp38-cp38-manylinux1_x86_64.whl (25.7 MB)\n\u001b[K     |████████████████████████████████| 25.7 MB 1.9 MB/s \n\u001b[?25hRequirement already satisfied: future in /usr/lib/python3/dist-packages (from torch>=1.4.0->txtai) (0.18.2)\nRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.5->txtai) (7.0)\nRequirement already satisfied: fasteners>=0.14.1 in /usr/lib/python3/dist-packages (from pymagnitude-lite>=0.1.43->txtai) (0.14.1)\nCollecting xxhash>=1.0.1\n  Downloading xxhash-2.0.0-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n\u001b[K     |████████████████████████████████| 243 kB 2.0 MB/s \n\u001b[?25hCollecting lz4>=1.0.0\n  Downloading lz4-3.1.0-cp38-cp38-manylinux2010_x86_64.whl (1.8 MB)\n\u001b[K     |████████████████████████████████| 1.8 MB 2.0 MB/s \n\u001b[?25hRequirement already satisfied: pybind11>=2.0 in /home/antony/.local/lib/python3.8/site-packages (from hnswlib>=0.4.0->txtai) (2.5.0)\nRequirement already satisfied: setuptools>=0.7.0 in /usr/lib/python3/dist-packages (from fasttext>=0.9.2->txtai) (45.2.0)\nCollecting sentencepiece!=0.1.92\n  Downloading sentencepiece-0.1.91-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 2.0 MB/s \n\u001b[?25hCollecting filelock\n  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\nCollecting tokenizers==0.8.1.rc1\n  Downloading tokenizers-0.8.1rc1-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 2.2 MB/s \n\u001b[?25hRequirement already satisfied: packaging in /home/antony/.local/lib/python3.8/site-packages (from transformers>=3.0.2->txtai) (20.4)\nRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers>=3.0.2->txtai) (2.22.0)\nCollecting sacremoses\n  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n\u001b[K     |████████████████████████████████| 883 kB 2.0 MB/s \n\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/antony/.local/lib/python3.8/site-packages (from packaging->transformers>=3.0.2->txtai) (2.4.7)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from packaging->transformers>=3.0.2->txtai) (1.14.0)\nBuilding wheels for collected packages: sentence-transformers, annoy, nltk, hnswlib, fasttext, sacremoses\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.4-py3-none-any.whl size=99822 sha256=e1dae71d9024a6234e7350622cf443e915924b8be10f8a5af08c8947df1d97eb\n  Stored in directory: /home/antony/.cache/pip/wheels/04/b3/c1/719d094c255bb2b38185bf56252e4e5853a52ce53ff6af3488\n  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for annoy: filename=annoy-1.16.3-cp38-cp38-linux_x86_64.whl size=449348 sha256=63cea6d555db7a95860630d22a49f14540cf5e325033b0f7c6bed96758f4859e\n  Stored in directory: /home/antony/.cache/pip/wheels/93/66/00/3527630e17462dcb505b4688f787b40bc020268237d54e5e79\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=7f8fd07785eeced603a30cfba92a67a5227b107eca27c8302ef0bbdd87c9c990\n  Stored in directory: /home/antony/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n  Building wheel for hnswlib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.4.0-cp38-cp38-linux_x86_64.whl size=1447592 sha256=d49b63a94fb123cf3be300d9483a8c3e513c291177adccac0ef16fdd0525d931\n  Stored in directory: /home/antony/.cache/pip/wheels/9d/b8/4d/35d4a37d5541fc9e039cbcbae21904cc2bcdaa91a1e4020193\n  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4213792 sha256=00ecf2efc26307936bf1a46f73253daa95839fb4f59fabbf8c9ed1d29e6228b4\n  Stored in directory: /home/antony/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893260 sha256=e3b7d170827607c2578441c2195802ff55caf8f605de0e6077d5e8c202e2e2ee\n  Stored in directory: /home/antony/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\nSuccessfully built sentence-transformers annoy nltk hnswlib fasttext sacremoses\nInstalling collected packages: joblib, threadpoolctl, scipy, scikit-learn, torch, regex, tqdm, nltk, sentencepiece, filelock, tokenizers, sacremoses, transformers, sentence-transformers, annoy, xxhash, lz4, pymagnitude-lite, hnswlib, fasttext, faiss-cpu, txtai\nSuccessfully installed annoy-1.16.3 faiss-cpu-1.6.3 fasttext-0.9.2 filelock-3.0.12 hnswlib-0.4.0 joblib-0.16.0 lz4-3.1.0 nltk-3.5 pymagnitude-lite-0.1.143 regex-2020.7.14 sacremoses-0.0.43 scikit-learn-0.23.2 scipy-1.5.2 sentence-transformers-0.3.4 sentencepiece-0.1.91 threadpoolctl-2.1.0 tokenizers-0.8.1rc1 torch-1.6.0 tqdm-4.48.2 transformers-3.0.2 txtai-1.1.0 xxhash-2.0.0\nNote: you may need to restart the kernel to use updated packages.\n"
    }
   ],
   "source": [
    "pip install txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from txtai.embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = Embeddings({\"method\": \"transformers\", \"path\": \"sentence-transformers/bert-base-nli-mean-tokens\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [\"Montreal airport authority to lay off 172 people and cut capital spending \",\n",
    "            \"How Canada became the battleground for America's tech Cold War with China\",\n",
    "            \"Canadian economy posts record second-quarter decline on pandemic hit\",\n",
    "            \"Grocers fight back against calls for a government enforced business code of conduct\",\n",
    "            \"Foreign investors left holding the bag in U.S. commercial property crisis\"]\n",
    "queries= [\"foreign investment\", \"retail business\", \"regulatory barriers\", \"science and innovation \", \"travel industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2 0 1 3]\n[3 1]\n"
    }
   ],
   "source": [
    "def sim (query):\n",
    "    x=embeddings.similarity(query, sections)\n",
    "    # uid = np.argsort(-(embeddings.similarity(query, sections)))\n",
    "    uid = np.argsort(x)\n",
    "    print(uid)\n",
    "    # ranked = np.argsort(an_array)\n",
    "\n",
    "    largest_indices = uid[::-1][:2]\n",
    "    print(largest_indices)\n",
    "\n",
    "sim('regulatory barriers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Canadian economy posts record second-quarter decline on pandemic hit\nMontreal airport authority to lay off 172 people and cut capital spending \nHow Canada became the battleground for America's tech Cold War with China\nForeign investors left holding the bag in U.S. commercial property crisis\n"
    }
   ],
   "source": [
    "# using a key theme (query) to find articles \n",
    "def sim (query):\n",
    "    x=embeddings.similarity(query, sections)\n",
    "    uid = np.argsort(x)\n",
    "    for i in uid:\n",
    "        print(sections[i])\n",
    "\n",
    "sim('regulatory barriers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "column types:\none    object\ntwo    object\ndtype: object\n"
    }
   ],
   "source": [
    "# dataframe to list or array\n",
    "data_dict = {'one': pd.Series(['and drop', 'bob gilgof', 'carl sagan' , 'nuns on hills'], index=[1, 2, 3,4]),\n",
    "             'two': pd.Series(['Canadian economy', 'Montreal airport', 'How Canada', 'Development bank' ], index=[1, 2, 3, 4])}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(f\"column types:\\n{df.dtypes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['and drop' 'bob gilgof' 'carl sagan' 'nuns on hills']\n"
    }
   ],
   "source": [
    "col_one_list = df['one'].tolist()\n",
    "col_one_arr = df['one'].to_numpy()\n",
    "print(col_one_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[3 2 1 0]\n[0 1]\n"
    }
   ],
   "source": [
    "def sim (query):\n",
    "    x=embeddings.similarity(query, col_one_list)\n",
    "    # uid = np.argsort(-(embeddings.similarity(query, sections)))\n",
    "    uid = np.argsort(x)\n",
    "    print(uid)\n",
    "    # ranked = np.argsort(an_array)\n",
    "\n",
    "    largest_indices = uid[::-1][:2]\n",
    "    print(largest_indices)\n",
    "\n",
    "sim('regulatory barriers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}