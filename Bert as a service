#MUST be running on Python >= 3.5 with Tensorflow >= 1.1
#dwnld and uncompress into say /tmp/english_L-12_H-768_A-12/ 
#(from https://github.com/hanxiao/bert-as-service/blob/master/README.md#getting-started)

pip install tensorflow==1.10.0 #one point ten
pip install bert-serving-client bert-serving-server -U
pip install ftfy
pip install nltk
pip install --upgrade matplotlib
pip install seaborn
bert-serving-start -model_dir /home/antony/t2/uncased_L-12_H-768_A-12/ -num_worker=1 -max_seq_len=NONE
 
#-fixed_embed_length by bert-serving-start --help especially if you intend to use it as ELMo-like embedding.
#(or run the below code, from the bert repo : python example8.py -model_dir=uncased_L-12_H-768_A-12)

pip install bert-serving-client bert-serving-server -U

#on python  ...
import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
import sys
import tensorflow
import numpy as np
import nltk
nltk.download('punkt')
from nltk import tokenize
from bert_serving.server import BertServer # still need this?
from bert_serving.client import BertClient
bc = BertClient()
from nltk import tokenize
from ftfy import fix_text
bc.encode(['First do it', 'then do it right', 'then do it better'])

#Reading  paragraphs of text from csv file then and then encoding . 
#File contains a column, with 4 rows of paragraphs from 4 news article.
#Sentences per paragraph vary between 3 to 5. So vector dimensions vary from (768, 3) to (768, 5) 
def load_from_csv(file):
    df = pd.read_csv(file)
    df = df.rename(str.lower, axis='columns')
    df['paragraphs'] = df['paragraphs'].apply(lambda x: x.replace("'s", " " "s").replace("\n"," "))
    df['paragraphs'] = df['paragraphs'].apply(lambda x: tokenize.sent_tokenize(x))
    BertClient=bc
    df['encoded'] = df['paragraphs'].apply(lambda x: BertClient.encode(x))
    return df

df=load_from_csv('x.csv')
             
# Flattening out t# Flattening out above encoded vectors. Each then becomes of dimension(768,) . 
X1 = np.mean(df.iat[0,1], axis=0) 
X2 = np.mean(df.iat[1,1], axis=0) 
X3 = np.mean(df.iat[2,1], axis=0) 
X4 = np.mean(df.iat[3,1], axis=0) 
X5 = np.mean(df.iat[4,1], axis=0) 
X6 = np.mean(df.iat[5,1], axis=0)
X7 = np.mean(df.iat[6,1], axis=0)
X8 = np.mean(df.iat[7,1], axis=0)
X9 = np.mean(df.iat[8,1], axis=0)
X10 = np.mean(df.iat[9,1], axis=0)

#Flattened vectors are stacked to make one vector X of size (768,4). X is transposed to (4,768), to better suit PCA model
X=np.column_stack([X1,X2,X3,X4,X5,X6,X7,X8,X9,X10])
XT=X.T

#Parameters of PCA
feat_cols = [ 'pixel'+str(i) for i in range(XT.shape[1]) ]
df = pd.DataFrame(XT,columns=feat_cols)
#Labels to identify the vectors when projected onto PCA 
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
df['y'] = numbers
df['label'] = df['y'].apply(lambda i: str(i))
#PCA steps
np.random.seed(42)  
rndperm = np.random.permutation(df.shape[0])
pca = PCA(n_components=3)
pca_result = pca.fit_transform(df[feat_cols].values)
df['pca-one'] = pca_result[:,0]
df['pca-two'] = pca_result[:,1] 
df['pca-three'] = pca_result[:,2]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))

plt.figure(figsize=(16,10))
sns.scatterplot(
    x="pca-one", y="pca-two",
    hue="y",
    palette=sns.color_palette("hls", 10),
    data=df.loc[rndperm,:],
    legend="full",
    alpha=1.2
)
plt.show()

ax = plt.figure(figsize=(16,10)).gca(projection='3d')
ax.scatter(
    xs=df.loc[rndperm,:]["pca-one"], 
    ys=df.loc[rndperm,:]["pca-two"], 
    zs=df.loc[rndperm,:]["pca-three"], 
    c=df.loc[rndperm,:]["y"], 
    cmap='tab10'
)
ax.set_xlabel('pca-one')
ax.set_ylabel('pca-two')
ax.set_zlabel('pca-three')
plt.show()

ref:
https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b
