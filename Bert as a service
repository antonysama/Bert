#MUST be running on Python >= 3.5 with Tensorflow >= 1.1
#dwnld and uncompress into say /tmp/english_L-12_H-768_A-12/ 
#(from https://github.com/hanxiao/bert-as-service/blob/master/README.md#getting-started)
conda activate p36
pip install scikit-learn
pip install tensorflow==1.14.0 #one point fourteen
pip install bert-serving-client bert-serving-server -U
#pip install ftfy
pip install --upgrade nltk
pip install --upgrade matplotlib
pip install --upgrade seaborn
pip install --upgrade numpy as np
pip install --upgrade pandas as pd
bert-serving-start -model_dir /home/antony/t2/uncased_L-12_H-768_A-12/ -num_worker=1 -max_seq_len=15
 
#-fixed_embed_length by bert-serving-start --help especially if you intend to use it as ELMo-like embedding.
#(or run the below code, from the bert repo : python example8.py -model_dir=uncased_L-12_H-768_A-12)

pip install bert-serving-client bert-serving-server -U

conda activate p36
#on python  ...
import pandas as pd
import sklearn
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
import sys
import tensorflow
import numpy as np
importÂ nltk
nltk.download('punkt')
from bert_serving.server import BertServer # still need this?
from bert_serving.client import BertClient
bc = BertClient()
from nltk import tokenize
time_start = time.time()
import csv
#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
#tsne_results = tsne.fit_transform(data_subset)print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))
#from ftfy import fix_text

from sklearn.manifold import TSNE
bc.encode(['First do it', 'then do it right', 'then do it better'])
#or:
metadata = ['First do it', 'then do it right', 'then do it better']
df=bc.encode(metadata)

with open('output2.tsv', 'wt') as out_file:
    tsv_writer = csv.writer(out_file, delimiter='\t')
    for d in df:
      tsv_writer.writerow(X)
 
 with open('output3.tsv', 'wt') as out_file:
    tsv_writer = csv.writer(out_file, delimiter='\t')
    for meta in metadata:
      tsv_writer.writerow([meta])
      
output2 and output3 were uploaded to tensorflow embedded visualizer

#Reading  paragraphs of text from csv file then and then encoding . 
#File contains a column, with 4 rows of paragraphs from 4 news article.
#Sentences per paragraph vary between 3 to 5. So vector dimensions vary from (768, 3) to (768, 5) 
def load_from_csv(file):
    df = pd.read_csv(file)
    df = df.rename(str.lower, axis='columns')
    df['paragraphs'] = df['paragraphs'].apply(lambda x: x.replace("'s", " " "s").replace("\n"," "))
    df['paragraphs'] = df['paragraphs'].apply(lambda x: tokenize.sent_tokenize(x))
    df['encoded'] = df['paragraphs'].apply(lambda x: bc.encode(x))
    return df

df=load_from_csv('x.csv')
             
# Flattening out t# Flattening out above encoded vectors. Each then becomes of dimension(768,) . 
X1 = np.mean(df.iat[0,1], axis=0) 
X2 = np.mean(df.iat[1,1], axis=0) 
X3 = np.mean(df.iat[2,1], axis=0) 
X4 = np.mean(df.iat[3,1], axis=0) 
X5 = np.mean(df.iat[4,1], axis=0) 
X6 = np.mean(df.iat[5,1], axis=0)
X7 = np.mean(df.iat[6,1], axis=0)
X8 = np.mean(df.iat[7,1], axis=0)
X9 = np.mean(df.iat[8,1], axis=0)
X10 = np.mean(df.iat[9,1], axis=0)

#Flattened vectors are stacked to make one vector X of size (768,4). X is transposed to (4,768), to better suit PCA model
X=np.column_stack([X1,X2,X3,X4,X5,X6,X7,X8,X9,X10])
XT=X.T

#Parameters of PCA
feat_cols = [ 'pixel'+str(i) for i in range(XT.shape[1]) ]
df = pd.DataFrame(XT,columns=feat_cols)
#Labels to identify the vectors when projected onto PCA 
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
df['y'] = numbers
df['label'] = df['y'].apply(lambda i: str(i))
#PCA steps
np.random.seed(42)  
rndperm = np.random.permutation(df.shape[0])
pca = PCA(n_components=3)
pca_result = pca.fit_transform(df[feat_cols].values)
df['pca-one'] = pca_result[:,0]
df['pca-two'] = pca_result[:,1] 
df['pca-three'] = pca_result[:,2]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))

plt.figure(figsize=(16,10))
sns.scatterplot(
    x="pca-one", y="pca-two",
    hue="y",
    palette=sns.color_palette("hls", 10),
    data=df.loc[rndperm,:],
    legend="full",
    alpha=1
)
plt.show()

ax = plt.figure(figsize=(16,10)).gca(projection='3d')
ax.scatter(
    xs=df.loc[rndperm,:]["pca-one"], 
    ys=df.loc[rndperm,:]["pca-two"], 
    zs=df.loc[rndperm,:]["pca-three"], 
    c=df.loc[rndperm,:]["y"], 
    cmap='tab10'
)
ax.set_xlabel('pca-one')
ax.set_ylabel('pca-two')
ax.set_zlabel('pca-three')
plt.show()

N=1000
df_subset = df.loc[rndperm[:N],:].copy()
data_subset = df_subset[feat_cols].values
pca = PCA(n_components=3)
pca_result = pca.fit_transform(data_subset)
df_subset['pca-one'] = pca_result[:,0]
df_subset['pca-two'] = pca_result[:,1] 
df_subset['pca-three'] = pca_result[:,2]
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))

time_start = time.time()
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(data_subset)
print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))

df_subset['tsne-2d-one'] = tsne_results[:,0]
df_subset['tsne-2d-two'] = tsne_results[:,1]
plt.figure(figsize=(16,10))
sns.scatterplot(
    x="tsne-2d-one", y="tsne-2d-two",
    hue="y",
    palette=sns.color_palette("hls", 10),
    data=df_subset,
    legend="full",
    alpha=1
)

ref:
https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b
