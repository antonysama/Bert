# the foll. is on Ubuntu bash : 
cd ~/environments/k/
bert-serving-start -model_dir=uncased_L-12_H-768_A-12

# if you want to ELMo-like contextual word embedding set -pooling_strategy NONE as so :
bert-serving-start -pooling_strategy NONE -model_dir=uncased_L-12_H-768_A-12

# the foll. is on Python :
Python
import json
import os
import time
import GPUtil
import tensorflow as tf
from bert_serving.server import BertServer

# If you want embeddings that corresponds to every token, you can simply use slice index as follows:
max_seq_len = 25
pooling_strategy = NONE

# First, make json text with one of the following before making bert client :
import json

#Reading downloaded json file (of sz 950bytes)
with open('rows.json', 'r') as f:
        data = f.read().strip();
        
str = data.replace("\'", "\"") # fixes error "missing dbl quotes". Then went to data node below.
$ pip install jsbeautifier   # you can also use jsbeautifier, which is less strict
$ js-beautify file.js   

# Or,
import requests
from pprint import pprint
with open('data.json') as f:
    data = json.load(f)
    
train_fp = str(data)

Or,
import urllib.request
connection = urllib.request.urlopen('https://jsonplaceholder.typicode.com/todos')
js = connection.read()   #json encoded as uitf-8
info = json.loads(js.decode("utf-8"))  # uif parsed  into a string
train_fp = json.loads(json.dumps(info)) #json dump then load

Or,
response = requests.get('https://jsonplaceholder.typicode.com/todos')
#train_fp  = json.loads(response.content)
js = response.read()   #json encoded at uitf-8
#train_fp = json.loads(js.decode("utf-8"))  #parse it into a string
train_fp = json.dumps(js , indent=4)

Or,
import requests
f=requests.get('https://gist.githubusercontent.com/planetoftheweb/98f35786733c8cccf81e/raw/f3dad774ed1fe20b36011b1261bb392ee759b867/data.json',params=
{"$filter":"Path eq '/xxxxxx/'"}).json()

with open('data.json', 'w') as out:
        json.dump(list(f), out, ensure_ascii=False, indent=4)
    
# Also make the following before making Bert Client
batch_size = 256
num_parallel_calls = 2
num_clients = 5  
os.environ['CUDA_VISIBLE_DEVICES'] = str(GPUtil.getFirstAvailable())

# Make Bert client w. one of the following :
from bert_serving.client import BertClient
bc = BertClient()

# Or,
bc = [BertClient(show_server_config=False) for _ in range(num_clients)]

# Or,
from bert_serving.client import ConcurrentBertClient
bc = ConcurrentBertClient()


# If you want to use your own tokenizer to segment sentences instead of the default one from BERTencode(is_tokenized=True) on the client slide as follows:
texts2 = [s.split() for s in texts]
bc.encode(texts2, is_tokenized=True)  # remember to first run bc = BertClient()

# After making bc do the following to make data iteration :
def get_encodes(x):
    # x is `batch_size` of lines, each of which is a json object
    samples = [json.loads(l) for l in x]
    text = [s['fact'][-50:] for s in samples]
    features = bc.encode(text)
    labels = [0 for _ in text]
    return features, labels

# If you want GPU growth use the following three lines:
config = tf.ConfigProto()
config.gpu_options.allow_growth = True 
session = tf.Session(config=config)

data_node = (tf.data.TextLineDataset(str).batch(batch_size)
             .map(lambda x: tf.py_func(get_encodes, [x], [tf.float32, tf.int64], name='bert_client'),
                  num_parallel_calls=num_parallel_calls)
             .map(lambda x, y: {'feature': x, 'label': y})
             .make_one_shot_iterator().get_next())

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    cnt, num_samples, start_t = 0, 0, time.perf_counter()
    while True:
        x = sess.run(data_node)
        cnt += 1
        num_samples += x['feature'].shape[0]
        if cnt % 10 == 0:
            time_used = time.perf_counter() - start_t
            print('data speed: %d/s' % int(num_samples / time_used))
            cnt, num_samples, start_t = 0, 0, time.perf_counter()

# Errors & how I fixed them:

tensorflow.python.framework.errors_... No such file or directory..	[fixed with naming the "train_fp.txt" in the last few lines]
tensorflow.python.framework.errors_...OutOfRangeError: End of sequence  [fixed w. more json data]
JSONDecodeError: Expecting value: line 1 column 1 ...[Fixed by opening a good json and running it through str = data.replace("\'", "\"")]
tensorflow.python.framework.errors_impl.InvalidArgumentError:..?
